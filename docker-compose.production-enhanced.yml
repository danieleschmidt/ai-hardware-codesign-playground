version: '3.8'

# AI Hardware Co-Design Playground - Enhanced Production Docker Compose
# Comprehensive production deployment with HA, monitoring, security, and compliance

services:
  # ============================================================================
  # Application Services
  # ============================================================================
  
  # Main API service with load balancing
  codesign-api:
    build:
      context: .
      dockerfile: docker/production-enhanced.dockerfile
      target: production
      args:
        - BUILD_DATE=${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        - VCS_REF=${VCS_REF:-$(git rev-parse HEAD)}
        - VERSION=${VERSION:-latest}
        - BUILD_ENV=production
    image: ${REGISTRY:-localhost}/codesign-playground:${VERSION:-latest}
    container_name: codesign-api-main
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: start-first
    environment:
      # Core configuration
      - ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG=false
      - PYTHONPATH=/app/backend
      
      # Database configuration
      - POSTGRES_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-primary:5432/codesign_db
      - POSTGRES_READ_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-replica:5432/codesign_db
      - DATABASE_POOL_SIZE=20
      - DATABASE_MAX_OVERFLOW=30
      - DATABASE_POOL_TIMEOUT=30
      
      # Redis configuration
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis-sentinel:26379
      - REDIS_CLUSTER_MODE=sentinel
      - REDIS_SENTINEL_SERVICE_NAME=mymaster
      - REDIS_POOL_SIZE=20
      
      # Security configuration
      - SECRET_KEY=${SECRET_KEY}
      - JWT_SECRET=${JWT_SECRET}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - SECURITY_HARDENED=true
      - ENABLE_RATE_LIMITING=true
      - RATE_LIMIT_PER_MINUTE=100
      - ENABLE_REQUEST_VALIDATION=strict
      - MAX_REQUEST_SIZE=100MB
      - ALLOWED_HOSTS=${ALLOWED_HOSTS:-localhost,127.0.0.1}
      
      # Compliance configuration
      - COMPLIANCE_REGION=${COMPLIANCE_REGION:-global}
      - GDPR_ENABLED=true
      - CCPA_ENABLED=true
      - PDPA_ENABLED=true
      - AUDIT_LOGGING_ENABLED=true
      - DATA_RETENTION_DAYS=2555  # 7 years
      
      # Performance configuration
      - WORKERS=${API_WORKERS:-4}
      - MAX_REQUESTS=10000
      - MAX_REQUESTS_JITTER=1000
      - TIMEOUT=120
      - KEEPALIVE=5
      - CACHE_SIZE_MB=512
      - ENABLE_COMPRESSION=true
      
      # Monitoring configuration
      - ENABLE_MONITORING=true
      - ENABLE_METRICS=true
      - ENABLE_TRACING=true
      - ENABLE_APM=true
      - METRICS_PORT=9090
      - SENTRY_DSN=${SENTRY_DSN}
      - SENTRY_ENVIRONMENT=production
      
      # Feature flags
      - ENABLE_CIRCUIT_BREAKER=true
      - ENABLE_HEALTH_CHECKS=true
      - ENABLE_BACKUP=true
      - ENABLE_EDGE_CACHING=true
    volumes:
      - ./data:/app/data:rw
      - ./logs:/app/logs:rw
      - ./uploads:/app/uploads:rw
      - ./backups:/app/backups:rw
      - ./compliance:/app/compliance:rw
      - compliance_data:/app/compliance_db:rw
      - api_cache:/app/cache:rw
    networks:
      - codesign-network
      - monitoring-network
      - security-network
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/detailed"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    security_opt:
      - no-new-privileges:true
    read_only: false
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    ulimits:
      nproc: 65535
      nofile:
        soft: 20000
        hard: 40000
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "10"
        compress: "true"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.codesign-api.rule=Host(`${DOMAIN:-localhost}`)"
      - "traefik.http.routers.codesign-api.tls=true"
      - "traefik.http.services.codesign-api.loadbalancer.server.port=8000"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9090"

  # Celery workers for background tasks
  celery-worker:
    build:
      context: .
      dockerfile: docker/production-enhanced.dockerfile
      target: worker
      args:
        - BUILD_DATE=${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        - VCS_REF=${VCS_REF:-$(git rev-parse HEAD)}
        - VERSION=${VERSION:-latest}
    image: ${REGISTRY:-localhost}/codesign-playground-worker:${VERSION:-latest}
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G
    environment:
      - ENV=production
      - PYTHONPATH=/app/backend
      - POSTGRES_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-primary:5432/codesign_db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis-sentinel:26379
      - CELERY_WORKERS=4
      - CELERY_MAX_TASKS_PER_CHILD=1000
      - CELERY_TIME_LIMIT=3600
      - CELERY_SOFT_TIME_LIMIT=3300
      - ENABLE_MONITORING=true
      - SENTRY_DSN=${SENTRY_DSN}
    volumes:
      - ./data:/app/data:rw
      - ./logs:/app/logs:rw
      - ./compliance:/app/compliance:rw
      - worker_cache:/app/cache:rw
    networks:
      - codesign-network
    depends_on:
      - postgres-primary
      - redis-master
      - codesign-api
    healthcheck:
      test: ["CMD", "celery", "-A", "codesign_playground.tasks.celery", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  # Celery scheduler
  celery-scheduler:
    build:
      context: .
      dockerfile: docker/production-enhanced.dockerfile
      target: scheduler
    restart: unless-stopped
    environment:
      - ENV=production
      - PYTHONPATH=/app/backend
      - POSTGRES_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-primary:5432/codesign_db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis-sentinel:26379
    volumes:
      - ./logs:/app/logs:rw
      - scheduler_data:/app/celerybeat:rw
    networks:
      - codesign-network
    depends_on:
      - postgres-primary
      - redis-master

  # ============================================================================
  # Database Services - High Availability PostgreSQL
  # ============================================================================
  
  # Primary PostgreSQL database
  postgres-primary:
    image: postgres:15-alpine
    container_name: postgres-primary
    restart: unless-stopped
    environment:
      - POSTGRES_DB=codesign_db
      - POSTGRES_USER=codesign
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - PGDATA=/var/lib/postgresql/data/pgdata
      # Performance tuning
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      - POSTGRES_MAX_CONNECTIONS=200
      - POSTGRES_SHARED_BUFFERS=512MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=2GB
      - POSTGRES_MAINTENANCE_WORK_MEM=128MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=32MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
      - POSTGRES_RANDOM_PAGE_COST=1.1
      - POSTGRES_EFFECTIVE_IO_CONCURRENCY=200
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - ./scripts/postgres/primary-init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - ./config/postgres/postgresql-primary.conf:/etc/postgresql/postgresql.conf:ro
      - ./config/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - postgres_backups:/var/lib/postgresql/backups
    networks:
      - codesign-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U codesign -d codesign_db -h localhost"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c hba_file=/etc/postgresql/pg_hba.conf

  # PostgreSQL read replica
  postgres-replica:
    image: postgres:15-alpine
    container_name: postgres-replica
    restart: unless-stopped
    environment:
      - PGUSER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_MASTER_USER=replicator
      - POSTGRES_MASTER_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - POSTGRES_MASTER_HOST=postgres-primary
      - POSTGRES_MASTER_PORT=5432
      - POSTGRES_DB=codesign_db
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
      - ./scripts/postgres/replica-init.sh:/docker-entrypoint-initdb.d/01-replica-init.sh:ro
    networks:
      - codesign-network
    depends_on:
      postgres-primary:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================================================
  # Redis Services - High Availability with Sentinel
  # ============================================================================
  
  # Redis master
  redis-master:
    image: redis:7-alpine
    container_name: redis-master
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --appendonly yes
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --tcp-keepalive 300
      --timeout 0
      --tcp-backlog 511
      --databases 16
      --stop-writes-on-bgsave-error yes
      --rdbcompression yes
      --rdbchecksum yes
      --dir /data
    volumes:
      - redis_master_data:/data
    networks:
      - codesign-network
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.5G
        reservations:
          cpus: '0.2'
          memory: 512M

  # Redis replica
  redis-replica:
    image: redis:7-alpine
    container_name: redis-replica
    restart: unless-stopped
    command: >
      redis-server
      --replicaof redis-master 6379
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --appendonly yes
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_replica_data:/data
    networks:
      - codesign-network
    depends_on:
      redis-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Sentinel for automatic failover
  redis-sentinel:
    image: redis:7-alpine
    container_name: redis-sentinel
    restart: unless-stopped
    command: >
      redis-sentinel /etc/redis/sentinel.conf
      --sentinel announce-ip redis-sentinel
      --sentinel announce-port 26379
    volumes:
      - ./config/redis/sentinel.conf:/etc/redis/sentinel.conf:ro
    networks:
      - codesign-network
    depends_on:
      - redis-master
      - redis-replica
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "26379", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================================================
  # Load Balancer and Reverse Proxy
  # ============================================================================
  
  # Traefik reverse proxy with automatic SSL
  traefik:
    image: traefik:v2.10
    container_name: traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--api.debug=false"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--global.checknewversion=false"
      - "--global.sendanonymoususage=false"
      - "--metrics.prometheus=true"
      - "--accesslog=true"
      - "--log.level=INFO"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik_letsencrypt:/letsencrypt
      - ./logs/traefik:/var/log/traefik
    networks:
      - codesign-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.traefik.tls=true"
      - "traefik.http.routers.traefik.tls.certresolver=letsencrypt"
      - "traefik.http.services.traefik.loadbalancer.server.port=8080"

  # ============================================================================
  # Monitoring and Observability Stack
  # ============================================================================
  
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--storage.tsdb.retention.time=90d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.external-url=http://prometheus.${DOMAIN:-localhost}'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    networks:
      - monitoring-network
      - codesign-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel
      - GF_RENDERING_SERVER_URL=http://grafana-renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
      - GF_LOG_FILTERS=rendering:debug
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - monitoring-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.grafana.tls=true"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

  # Grafana Image Renderer
  grafana-renderer:
    image: grafana/grafana-image-renderer:latest
    container_name: grafana-renderer
    restart: unless-stopped
    environment:
      - ENABLE_METRICS=true
      - HTTP_PORT=8081
    networks:
      - monitoring-network

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    restart: unless-stopped
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    volumes:
      - jaeger_data:/tmp
    networks:
      - monitoring-network
      - codesign-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:16686"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.jaeger.tls=true"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"

  # ElasticSearch for log storage
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - cluster.name=codesign-logs
      - node.name=elasticsearch-01
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - monitoring-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Kibana for log visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - XPACK_SECURITY_ENABLED=false
    networks:
      - monitoring-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kibana.rule=Host(`kibana.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.kibana.tls=true"
      - "traefik.http.services.kibana.loadbalancer.server.port=5601"

  # Fluent Bit for log collection
  fluent-bit:
    image: fluent/fluent-bit:latest
    container_name: fluent-bit
    restart: unless-stopped
    volumes:
      - ./monitoring/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./monitoring/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./logs:/var/log/app:ro
      - /var/log:/var/log/host:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - monitoring-network
      - codesign-network
    depends_on:
      - elasticsearch

  # ============================================================================
  # Security and Compliance Services
  # ============================================================================
  
  # Vault for secrets management
  vault:
    image: vault:latest
    container_name: vault
    restart: unless-stopped
    cap_add:
      - IPC_LOCK
    environment:
      - VAULT_DEV_ROOT_TOKEN_ID=${VAULT_ROOT_TOKEN}
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
      - VAULT_ADDR=http://127.0.0.1:8200
    volumes:
      - vault_data:/vault/data
      - ./config/vault:/vault/config:ro
    networks:
      - security-network
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.vault.rule=Host(`vault.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.vault.tls=true"
      - "traefik.http.services.vault.loadbalancer.server.port=8200"

  # Security scanner
  trivy-scanner:
    image: aquasec/trivy:latest
    container_name: trivy-scanner
    restart: "no"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/reports:/reports
      - trivy_cache:/root/.cache
    networks:
      - security-network
    profiles:
      - security-scan
    command: >
      sh -c '
        trivy image --format json --output /reports/api-scan.json ${REGISTRY:-localhost}/codesign-playground:${VERSION:-latest} &&
        trivy image --format json --output /reports/worker-scan.json ${REGISTRY:-localhost}/codesign-playground-worker:${VERSION:-latest} &&
        trivy fs --format json --output /reports/filesystem-scan.json /
      '

  # Compliance auditor
  compliance-auditor:
    build:
      context: .
      dockerfile: docker/compliance.dockerfile
    container_name: compliance-auditor
    restart: unless-stopped
    environment:
      - POSTGRES_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-primary:5432/codesign_db
      - AUDIT_LEVEL=detailed
      - COMPLIANCE_REGIONS=EU,US,APAC
      - RETENTION_PERIOD_DAYS=2555
    volumes:
      - ./compliance:/app/compliance:rw
      - compliance_data:/app/data:rw
    networks:
      - codesign-network
      - security-network
    depends_on:
      - postgres-primary

  # ============================================================================
  # Backup and Recovery Services
  # ============================================================================
  
  # Automated backup service
  backup-service:
    build:
      context: .
      dockerfile: docker/backup.dockerfile
    container_name: backup-service
    restart: unless-stopped
    environment:
      - POSTGRES_URL=postgresql://codesign:${POSTGRES_PASSWORD}@postgres-primary:5432/codesign_db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis-master:6379
      - BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
      - BACKUP_RETENTION_DAYS=90
      - S3_BACKUP_ENABLED=${S3_BACKUP_ENABLED:-false}
      - S3_BUCKET=${S3_BACKUP_BUCKET}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - ENCRYPTION_ENABLED=true
      - ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY}
    volumes:
      - postgres_backups:/backups/postgres
      - redis_master_data:/data/redis:ro
      - ./backups:/backups/local
      - compliance_data:/data/compliance:ro
    networks:
      - codesign-network
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-master:
        condition: service_healthy
    profiles:
      - backup

# ============================================================================
# Network Configuration
# ============================================================================
networks:
  codesign-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: codesign0
    ipam:
      config:
        - subnet: 172.20.0.0/16
  
  monitoring-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: monitoring0
    ipam:
      config:
        - subnet: 172.21.0.0/16
  
  security-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: security0
    ipam:
      config:
        - subnet: 172.22.0.0/16

# ============================================================================
# Volume Configuration
# ============================================================================
volumes:
  # Database volumes
  postgres_primary_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/postgres-primary
  
  postgres_replica_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/postgres-replica
  
  postgres_backups:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_PATH:-./backups}/postgres

  # Redis volumes
  redis_master_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/redis-master
  
  redis_replica_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/redis-replica

  # Application volumes
  api_cache:
    driver: local
  worker_cache:
    driver: local
  scheduler_data:
    driver: local

  # Monitoring volumes
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/prometheus
  
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/grafana
  
  jaeger_data:
    driver: local
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/elasticsearch

  # Security volumes
  vault_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/vault
  
  trivy_cache:
    driver: local

  # SSL and certificates
  traefik_letsencrypt:
    driver: local

  # Compliance and audit data
  compliance_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/compliance