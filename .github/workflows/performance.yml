name: Performance Testing

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          npm ci
      
      - name: Install Performance Testing Tools
        run: |
          npm install -g artillery
          pip install locust pytest-benchmark
      
      - name: Build Application
        run: |
          npm run build:frontend
          npm run build:backend
      
      - name: Start Application
        run: |
          npm run start &
          sleep 30
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
          REDIS_URL: redis://localhost:6379/0
          NODE_ENV: production
      
      - name: Run API Load Tests
        run: |
          artillery run tests/performance/artillery.yml --output artillery-report.json
      
      - name: Run Python Performance Tests
        run: |
          pytest tests/performance/ -v --benchmark-json=python-benchmark.json
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/perf_test_db
          REDIS_URL: redis://localhost:6379/0
      
      - name: Run Hardware Simulation Performance Tests
        run: |
          pytest tests/ -m "performance and hardware" -v --junit-xml=hardware-perf-results.xml
        env:
          HARDWARE_TOOLS_AVAILABLE: "true"
      
      - name: Generate Performance Report
        run: |
          echo "# Performance Test Results - $(date)" > performance-report.md
          echo "" >> performance-report.md
          echo "## Load Testing Results" >> performance-report.md
          
          if [ -f artillery-report.json ]; then
            echo "- Artillery load test completed" >> performance-report.md
            echo "- See attached artifacts for detailed results" >> performance-report.md
          fi
          
          if [ -f python-benchmark.json ]; then
            echo "## Python Benchmark Results" >> performance-report.md
            echo "- Python performance benchmarks completed" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          echo "**Generated on:** $(date)" >> performance-report.md
      
      - name: Upload Performance Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-results
          path: |
            artillery-report.json
            python-benchmark.json
            hardware-perf-results.xml
            performance-report.md
      
      - name: Comment PR with Performance Results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('performance-report.md')) {
              const report = fs.readFileSync('performance-report.md', 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## âš¡ Performance Test Results\n\n${report}`
              });
            }

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    timeout-minutes: 30
    
    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark
      
      - name: Run PR Benchmarks
        run: |
          pytest tests/benchmarks/ --benchmark-json=pr-benchmark.json
      
      - name: Checkout Base Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.base.sha }}
      
      - name: Run Base Benchmarks
        run: |
          pip install -r requirements.txt
          pytest tests/benchmarks/ --benchmark-json=base-benchmark.json
      
      - name: Compare Benchmarks
        run: |
          python scripts/compare-benchmarks.py base-benchmark.json pr-benchmark.json > benchmark-comparison.md
      
      - name: Upload Benchmark Comparison
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-comparison
          path: benchmark-comparison.md